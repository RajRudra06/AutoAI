{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPLEiJ96vNHJ5ole5q/j8YY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mZH7uSPENztY","executionInfo":{"status":"ok","timestamp":1765772079114,"user_tz":-330,"elapsed":2021,"user":{"displayName":"Rudra Rajpurohit","userId":"01182349033982099683"}},"outputId":"8804c3ae-8a46-4013-f574-b270ad2fc2ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ All packages ready!\n","NumPy: 2.0.2\n","Pandas: 2.2.2\n","Scikit-learn: 1.6.1\n"]}],"source":["# Colab already has numpy, pandas, scikit-learn, matplotlib, seaborn\n","# Just verify versions\n","import numpy as np\n","import pandas as pd\n","import sklearn\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","print(\"‚úÖ All packages ready!\")\n","print(f\"NumPy: {np.__version__}\")\n","print(f\"Pandas: {pd.__version__}\")\n","print(f\"Scikit-learn: {sklearn.__version__}\")\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3BKhVDtZN414","executionInfo":{"status":"ok","timestamp":1765772212214,"user_tz":-330,"elapsed":24218,"user":{"displayName":"Rudra Rajpurohit","userId":"01182349033982099683"}},"outputId":"44c2d359-4a74-48da-99d5-42e89a3d3fef"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","BASE_DIR = \"/content/drive/MyDrive/iForestAutoAI\"\n","os.makedirs(BASE_DIR, exist_ok=True)"],"metadata":{"id":"gWIS308HOFOG","executionInfo":{"status":"ok","timestamp":1765772296734,"user_tz":-330,"elapsed":293,"user":{"displayName":"Rudra Rajpurohit","userId":"01182349033982099683"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Generate realistic synthetic automotive telemetry data\n","\"\"\"\n","import numpy as np\n","import pandas as pd\n","from datetime import datetime, timedelta\n","import json\n","\n","# Set seed for reproducibility\n","np.random.seed(42)\n","\n","\n","def generate_fleet_telemetry(\n","    n_vehicles=250,\n","    days_per_vehicle=30,\n","    readings_per_day=4,\n","    failure_rate=0.05\n","):\n","    \"\"\"\n","    Generate synthetic automotive telemetry data.\n","\n","    Args:\n","        n_vehicles: Number of vehicles in fleet\n","        days_per_vehicle: Days of history per vehicle\n","        readings_per_day: Telemetry readings per day\n","        failure_rate: Proportion of vehicles with issues (5%)\n","\n","    Returns:\n","        DataFrame with telemetry data\n","    \"\"\"\n","\n","    print(f\"üöó Generating data for {n_vehicles} vehicles...\")\n","    print(f\"   {days_per_vehicle} days √ó {readings_per_day} readings/day = {days_per_vehicle * readings_per_day} readings per vehicle\")\n","\n","    all_data = []\n","    n_failing = int(n_vehicles * failure_rate)\n","\n","    # Randomly select which vehicles will have issues\n","    failing_vehicle_ids = np.random.choice(n_vehicles, n_failing, replace=False)\n","\n","    for vehicle_idx in range(n_vehicles):\n","        vehicle_id = f\"VEH_{vehicle_idx + 1:04d}\"\n","        is_failing = vehicle_idx in failing_vehicle_ids\n","\n","        # Vehicle-specific baseline (some vehicles run hotter, etc.)\n","        vehicle_baseline = {\n","            'engine_temp_offset': np.random.normal(0, 3),\n","            'oil_pressure_offset': np.random.normal(0, 2),\n","            'battery_voltage_offset': np.random.normal(0, 0.1),\n","            'driving_style': np.random.choice(['gentle', 'normal', 'aggressive'], p=[0.2, 0.6, 0.2])\n","        }\n","\n","        # Generate readings\n","        for day in range(days_per_vehicle):\n","            for reading_idx in range(readings_per_day):\n","\n","                # Time progression (0 to 1 over the period)\n","                time_progress = (day * readings_per_day + reading_idx) / (days_per_vehicle * readings_per_day)\n","\n","                # Degradation factor (failing vehicles degrade faster)\n","                if is_failing:\n","                    degradation = time_progress * np.random.uniform(1.5, 2.5)\n","                else:\n","                    degradation = time_progress * np.random.uniform(0.1, 0.3)\n","\n","                # Timestamp\n","                timestamp = datetime(2024, 1, 1) + timedelta(\n","                    days=day,\n","                    hours=reading_idx * 6  # Readings at 0h, 6h, 12h, 18h\n","                )\n","\n","                # === ENGINE TEMPERATURE ===\n","                # Baseline: 85-95¬∞C normal operating temp\n","                engine_temp_base = 90 + vehicle_baseline['engine_temp_offset']\n","                engine_temp_noise = np.random.normal(0, 2)\n","                engine_temp_degradation = degradation * 15  # Up to +15¬∞C when failing\n","\n","                # Time of day effect (hotter in afternoon)\n","                hour = timestamp.hour\n","                time_of_day_effect = 3 * np.sin((hour - 6) * np.pi / 12)  # Peak at 6pm\n","\n","                engine_temp_c = engine_temp_base + engine_temp_noise + engine_temp_degradation + time_of_day_effect\n","                engine_temp_c = np.clip(engine_temp_c, 70, 120)\n","\n","                # === OIL PRESSURE ===\n","                # Baseline: 35-45 psi normal\n","                oil_pressure_base = 40 + vehicle_baseline['oil_pressure_offset']\n","                oil_pressure_noise = np.random.normal(0, 1.5)\n","                oil_pressure_degradation = degradation * -10  # Drops when failing\n","\n","                oil_pressure_psi = oil_pressure_base + oil_pressure_noise + oil_pressure_degradation\n","                oil_pressure_psi = np.clip(oil_pressure_psi, 15, 50)\n","\n","                # === BRAKE PAD THICKNESS ===\n","                # New: 10mm, Replace at 2mm\n","                brake_pad_base = 10\n","                brake_wear_rate = 0.01 if vehicle_baseline['driving_style'] == 'gentle' else \\\n","                                  0.015 if vehicle_baseline['driving_style'] == 'normal' else 0.025\n","\n","                if is_failing:\n","                    brake_wear_rate *= 2  # Faster wear\n","\n","                brake_pad_mm = brake_pad_base - (day * brake_wear_rate)\n","                brake_pad_mm += np.random.normal(0, 0.1)  # Measurement noise\n","                brake_pad_mm = np.clip(brake_pad_mm, 1, 10)\n","\n","                # === BATTERY VOLTAGE ===\n","                # Healthy: 12.4-12.8V, Weak: <12.0V\n","                battery_voltage_base = 12.6 + vehicle_baseline['battery_voltage_offset']\n","                battery_voltage_noise = np.random.normal(0, 0.05)\n","                battery_voltage_degradation = degradation * -0.4\n","\n","                battery_voltage_v = battery_voltage_base + battery_voltage_noise + battery_voltage_degradation\n","                battery_voltage_v = np.clip(battery_voltage_v, 11.0, 13.0)\n","\n","                # === TIRE PRESSURES ===\n","                # Recommended: 32 psi, Variance between tires\n","                tire_base = 32\n","                tire_pressures_psi = [\n","                    tire_base + np.random.normal(0, 1.5) for _ in range(4)\n","                ]\n","                tire_pressures_psi = [np.clip(p, 25, 38) for p in tire_pressures_psi]\n","\n","                # === ENGINE RPM ===\n","                # Idle: ~800, Highway: ~2500\n","                if reading_idx in [0, 3]:  # Night/early morning - lower usage\n","                    rpm_base = np.random.normal(1200, 300)\n","                else:\n","                    rpm_base = np.random.normal(2000, 500)\n","                engine_rpm = np.clip(rpm_base, 700, 6000)\n","\n","                # === COOLANT TEMPERATURE ===\n","                # Usually tracks engine temp but slightly lower\n","                coolant_temp_c = engine_temp_c - np.random.uniform(3, 8)\n","                coolant_temp_c = np.clip(coolant_temp_c, 65, 110)\n","\n","                # === FUEL CONSUMPTION ===\n","                # L/100km - affected by driving style\n","                fuel_consumption_base = 8 if vehicle_baseline['driving_style'] == 'gentle' else \\\n","                                       10 if vehicle_baseline['driving_style'] == 'normal' else 13\n","                fuel_consumption = fuel_consumption_base + np.random.normal(0, 1)\n","                fuel_consumption = np.clip(fuel_consumption, 5, 20)\n","\n","                # === THROTTLE POSITION ===\n","                # 0-100% - correlates with driving style\n","                throttle_mean = 30 if vehicle_baseline['driving_style'] == 'gentle' else \\\n","                               50 if vehicle_baseline['driving_style'] == 'normal' else 70\n","                throttle_position_pct = np.random.normal(throttle_mean, 15)\n","                throttle_position_pct = np.clip(throttle_position_pct, 0, 100)\n","\n","                # === OPERATIONAL METRICS ===\n","                daily_miles = np.random.uniform(30, 120)\n","\n","                harsh_braking_events = 0\n","                if vehicle_baseline['driving_style'] == 'aggressive':\n","                    harsh_braking_events = np.random.poisson(3)\n","                elif vehicle_baseline['driving_style'] == 'normal':\n","                    harsh_braking_events = np.random.poisson(0.5)\n","\n","                harsh_acceleration_events = 0\n","                if vehicle_baseline['driving_style'] == 'aggressive':\n","                    harsh_acceleration_events = np.random.poisson(2.5)\n","                elif vehicle_baseline['driving_style'] == 'normal':\n","                    harsh_acceleration_events = np.random.poisson(0.3)\n","\n","                cold_starts = 1 if reading_idx == 0 else 0  # First reading of day\n","\n","                # === MAINTENANCE TRACKING ===\n","                miles_since_last_service = day * 50 + np.random.uniform(-10, 10)\n","\n","                # === AMBIENT CONDITIONS ===\n","                # Affects readings\n","                ambient_temp_c = 20 + 15 * np.sin((day / 365) * 2 * np.pi)  # Seasonal\n","                ambient_temp_c += 10 * np.sin((hour - 6) * np.pi / 12)  # Daily cycle\n","                ambient_temp_c = np.clip(ambient_temp_c, -5, 40)\n","\n","                # === VIBRATION (NEW) ===\n","                vibration_base = 0.5  # Low vibration is good\n","                vibration_degradation = degradation * 2  # Increases with wear\n","                vibration_level = vibration_base + vibration_degradation + np.random.normal(0, 0.1)\n","                vibration_level = np.clip(vibration_level, 0, 5)\n","\n","                # === TRANSMISSION TEMP (NEW) ===\n","                transmission_temp_base = 80\n","                transmission_temp = transmission_temp_base + degradation * 10 + np.random.normal(0, 3)\n","                transmission_temp = np.clip(transmission_temp, 70, 110)\n","\n","                # === CONSTRUCT RECORD ===\n","                record = {\n","                    'vehicle_id': vehicle_id,\n","                    'timestamp': timestamp,\n","                    'is_failing': is_failing,  # Ground truth label\n","\n","                    # Core sensors\n","                    'engine_temp_c': round(engine_temp_c, 1),\n","                    'oil_pressure_psi': round(oil_pressure_psi, 1),\n","                    'brake_pad_mm': round(brake_pad_mm, 2),\n","                    'battery_voltage_v': round(battery_voltage_v, 2),\n","\n","                    # Extended sensors\n","                    'tire_pressure_fl': round(tire_pressures_psi[0], 1),\n","                    'tire_pressure_fr': round(tire_pressures_psi[1], 1),\n","                    'tire_pressure_rl': round(tire_pressures_psi[2], 1),\n","                    'tire_pressure_rr': round(tire_pressures_psi[3], 1),\n","                    'engine_rpm': int(engine_rpm),\n","                    'coolant_temp_c': round(coolant_temp_c, 1),\n","                    'fuel_consumption_l_per_100km': round(fuel_consumption, 1),\n","                    'throttle_position_pct': round(throttle_position_pct, 1),\n","                    'vibration_level': round(vibration_level, 2),\n","                    'transmission_temp_c': round(transmission_temp, 1),\n","\n","                    # Operational\n","                    'daily_miles': round(daily_miles, 1),\n","                    'harsh_braking_events': harsh_braking_events,\n","                    'harsh_acceleration_events': harsh_acceleration_events,\n","                    'cold_starts': cold_starts,\n","                    'miles_since_last_service': round(miles_since_last_service, 1),\n","\n","                    # Environmental\n","                    'ambient_temp_c': round(ambient_temp_c, 1),\n","\n","                    # Metadata\n","                    'driving_style': vehicle_baseline['driving_style'],\n","                    'reading_of_day': reading_idx + 1\n","                }\n","\n","                all_data.append(record)\n","\n","        if (vehicle_idx + 1) % 50 == 0:\n","            print(f\"   ‚úì Generated {vehicle_idx + 1}/{n_vehicles} vehicles\")\n","\n","    df = pd.DataFrame(all_data)\n","\n","    print(f\"\\n‚úÖ Dataset generated:\")\n","    print(f\"   Total records: {len(df):,}\")\n","    print(f\"   Vehicles: {df['vehicle_id'].nunique()}\")\n","    print(f\"   Failing vehicles: {df[df['is_failing']]['vehicle_id'].nunique()} ({failure_rate*100}%)\")\n","    print(f\"   Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n","    print(f\"   Features: {len(df.columns)}\")\n","\n","    return df\n","\n","\n","def save_dataset(df, train_ratio=0.7, val_ratio=0.15):\n","    \"\"\"\n","    Split and save dataset into train/val/test sets.\n","    Split by vehicle (not by rows) to avoid leakage.\n","    \"\"\"\n","\n","    vehicles = df['vehicle_id'].unique()\n","    np.random.shuffle(vehicles)\n","\n","    n_train = int(len(vehicles) * train_ratio)\n","    n_val = int(len(vehicles) * val_ratio)\n","\n","    train_vehicles = vehicles[:n_train]\n","    val_vehicles = vehicles[n_train:n_train + n_val]\n","    test_vehicles = vehicles[n_train + n_val:]\n","\n","    train_df = df[df['vehicle_id'].isin(train_vehicles)]\n","    val_df = df[df['vehicle_id'].isin(val_vehicles)]\n","    test_df = df[df['vehicle_id'].isin(test_vehicles)]\n","\n","    print(f\"\\nüìä Dataset split:\")\n","    print(f\"   Train: {len(train_vehicles)} vehicles ({len(train_df):,} records)\")\n","    print(f\"   Val:   {len(val_vehicles)} vehicles ({len(val_df):,} records)\")\n","    print(f\"   Test:  {len(test_vehicles)} vehicles ({len(test_df):,} records)\")\n","\n","    # === SAVE TO GOOGLE DRIVE ===\n","    train_path = os.path.join(BASE_DIR, \"train_telemetry.csv\")\n","    val_path = os.path.join(BASE_DIR, \"val_telemetry.csv\")\n","    test_path = os.path.join(BASE_DIR, \"test_telemetry.csv\")\n","    meta_path = os.path.join(BASE_DIR, \"dataset_metadata.json\")\n","\n","    train_df.to_csv(train_path, index=False)\n","    val_df.to_csv(val_path, index=False)\n","    test_df.to_csv(test_path, index=False)\n","\n","    metadata = {\n","        'generated_at': datetime.now().isoformat(),\n","        'n_vehicles_total': len(vehicles),\n","        'n_vehicles_train': len(train_vehicles),\n","        'n_vehicles_val': len(val_vehicles),\n","        'n_vehicles_test': len(test_vehicles),\n","        'n_records_total': len(df),\n","        'failure_rate': float(df['is_failing'].mean()),\n","        'features': list(df.columns),\n","        'date_range': {\n","            'start': df['timestamp'].min().isoformat(),\n","            'end': df['timestamp'].max().isoformat()\n","        }\n","    }\n","\n","    with open(meta_path, \"w\") as f:\n","        json.dump(metadata, f, indent=2)\n","\n","    print(f\"\\n‚úÖ Saved to Google Drive:\")\n","    print(f\"   {train_path}\")\n","    print(f\"   {val_path}\")\n","    print(f\"   {test_path}\")\n","    print(f\"   {meta_path}\")\n","\n","    return train_df, val_df, test_df\n","\n","\n","\n","if __name__ == \"__main__\":\n","    print(\"=\" * 60)\n","    print(\"üöÄ SYNTHETIC AUTOMOTIVE TELEMETRY GENERATOR\")\n","    print(\"=\" * 60)\n","\n","    # Generate dataset\n","    df = generate_fleet_telemetry(\n","        n_vehicles=250,\n","        days_per_vehicle=30,\n","        readings_per_day=4,\n","        failure_rate=0.05\n","    )\n","\n","    # Split and save\n","    train_df, val_df, test_df = save_dataset(df)\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"üéâ DATA GENERATION COMPLETE!\")\n","    print(\"=\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6NunKI_0Oq8k","executionInfo":{"status":"ok","timestamp":1765772358646,"user_tz":-330,"elapsed":10144,"user":{"displayName":"Rudra Rajpurohit","userId":"01182349033982099683"}},"outputId":"2c3fcb99-db71-4d8f-cd00-172510b5b2f0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","üöÄ SYNTHETIC AUTOMOTIVE TELEMETRY GENERATOR\n","============================================================\n","üöó Generating data for 250 vehicles...\n","   30 days √ó 4 readings/day = 120 readings per vehicle\n","   ‚úì Generated 50/250 vehicles\n","   ‚úì Generated 100/250 vehicles\n","   ‚úì Generated 150/250 vehicles\n","   ‚úì Generated 200/250 vehicles\n","   ‚úì Generated 250/250 vehicles\n","\n","‚úÖ Dataset generated:\n","   Total records: 30,000\n","   Vehicles: 250\n","   Failing vehicles: 12 (5.0%)\n","   Date range: 2024-01-01 00:00:00 to 2024-01-30 18:00:00\n","   Features: 25\n","\n","üìä Dataset split:\n","   Train: 175 vehicles (21,000 records)\n","   Val:   37 vehicles (4,440 records)\n","   Test:  38 vehicles (4,560 records)\n","\n","‚úÖ Saved to Google Drive:\n","   /content/drive/MyDrive/iForestAutoAI/train_telemetry.csv\n","   /content/drive/MyDrive/iForestAutoAI/val_telemetry.csv\n","   /content/drive/MyDrive/iForestAutoAI/test_telemetry.csv\n","   /content/drive/MyDrive/iForestAutoAI/dataset_metadata.json\n","\n","============================================================\n","üéâ DATA GENERATION COMPLETE!\n","============================================================\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Extract features from raw telemetry for ML model\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from typing import Dict, List\n","import os\n","\n","# ======================================================\n","# GOOGLE DRIVE PATH CONFIG\n","# ======================================================\n","BASE_DIR = \"/content/drive/MyDrive/iForestAutoAI\"\n","\n","TRAIN_PATH = os.path.join(BASE_DIR, \"train_telemetry.csv\")\n","VAL_PATH   = os.path.join(BASE_DIR, \"val_telemetry.csv\")\n","TEST_PATH  = os.path.join(BASE_DIR, \"test_telemetry.csv\")\n","\n","\n","def extract_features_from_vehicle(vehicle_df: pd.DataFrame) -> Dict:\n","    \"\"\"\n","    Extract feature vector from a single vehicle's telemetry history.\n","    \"\"\"\n","\n","    vehicle_df = vehicle_df.sort_values('timestamp').reset_index(drop=True)\n","\n","    if len(vehicle_df) < 10:\n","        raise ValueError(f\"Insufficient data: only {len(vehicle_df)} records\")\n","\n","    latest = vehicle_df.iloc[-1]\n","    df_7d = vehicle_df.tail(28)\n","    df_all = vehicle_df\n","\n","    features = {}\n","\n","    # ===== CURRENT STATE =====\n","    features['engine_temp_c'] = latest['engine_temp_c']\n","    features['oil_pressure_psi'] = latest['oil_pressure_psi']\n","    features['brake_pad_mm'] = latest['brake_pad_mm']\n","    features['battery_voltage_v'] = latest['battery_voltage_v']\n","    features['vibration_level'] = latest['vibration_level']\n","    features['transmission_temp_c'] = latest['transmission_temp_c']\n","\n","    tire_pressures = [\n","        latest['tire_pressure_fl'],\n","        latest['tire_pressure_fr'],\n","        latest['tire_pressure_rl'],\n","        latest['tire_pressure_rr']\n","    ]\n","    features['tire_pressure_avg'] = np.mean(tire_pressures)\n","    features['tire_pressure_std'] = np.std(tire_pressures)\n","    features['tire_pressure_min'] = np.min(tire_pressures)\n","\n","    # ===== 7-DAY STATS =====\n","    features['engine_temp_mean_7d'] = df_7d['engine_temp_c'].mean()\n","    features['engine_temp_std_7d'] = df_7d['engine_temp_c'].std()\n","    features['engine_temp_max_7d'] = df_7d['engine_temp_c'].max()\n","    features['engine_temp_min_7d'] = df_7d['engine_temp_c'].min()\n","\n","    features['oil_pressure_mean_7d'] = df_7d['oil_pressure_psi'].mean()\n","    features['oil_pressure_std_7d'] = df_7d['oil_pressure_psi'].std()\n","    features['oil_pressure_min_7d'] = df_7d['oil_pressure_psi'].min()\n","\n","    features['battery_voltage_mean_7d'] = df_7d['battery_voltage_v'].mean()\n","    features['battery_voltage_min_7d'] = df_7d['battery_voltage_v'].min()\n","\n","    features['coolant_temp_mean_7d'] = df_7d['coolant_temp_c'].mean()\n","    features['coolant_temp_max_7d'] = df_7d['coolant_temp_c'].max()\n","\n","    features['vibration_mean_7d'] = df_7d['vibration_level'].mean()\n","    features['vibration_max_7d'] = df_7d['vibration_level'].max()\n","\n","    features['transmission_temp_mean_7d'] = df_7d['transmission_temp_c'].mean()\n","    features['transmission_temp_max_7d'] = df_7d['transmission_temp_c'].max()\n","\n","    # ===== 30-DAY TRENDS =====\n","    days_elapsed = max(\n","        (df_all.iloc[-1]['timestamp'] - df_all.iloc[0]['timestamp']).days, 1\n","    )\n","\n","    features['brake_wear_rate_30d'] = (\n","        df_all.iloc[0]['brake_pad_mm'] - df_all.iloc[-1]['brake_pad_mm']\n","    ) / days_elapsed\n","\n","    features['oil_pressure_drop_rate_30d'] = (\n","        df_all.iloc[0]['oil_pressure_psi'] - df_all.iloc[-1]['oil_pressure_psi']\n","    ) / days_elapsed\n","\n","    features['battery_voltage_drop_rate_30d'] = (\n","        df_all.iloc[0]['battery_voltage_v'] - df_all.iloc[-1]['battery_voltage_v']\n","    ) / days_elapsed\n","\n","    features['vibration_increase_rate_30d'] = (\n","        df_all.iloc[-1]['vibration_level'] - df_all.iloc[0]['vibration_level']\n","    ) / days_elapsed\n","\n","    # ===== OPERATIONAL =====\n","    features['miles_since_last_service'] = latest['miles_since_last_service']\n","    features['avg_daily_miles_30d'] = df_all['daily_miles'].mean()\n","    features['total_harsh_braking_7d'] = df_7d['harsh_braking_events'].sum()\n","    features['total_harsh_acceleration_7d'] = df_7d['harsh_acceleration_events'].sum()\n","    features['total_cold_starts_7d'] = df_7d['cold_starts'].sum()\n","    features['avg_fuel_consumption_7d'] = df_7d['fuel_consumption_l_per_100km'].mean()\n","    features['avg_rpm_7d'] = df_7d['engine_rpm'].mean()\n","    features['max_rpm_7d'] = df_7d['engine_rpm'].max()\n","\n","    # ===== DERIVED =====\n","    features['engine_stress_index'] = (\n","        features['engine_temp_c'] / max(features['oil_pressure_psi'], 1)\n","    )\n","\n","    features['cooling_efficiency'] = (\n","        features['engine_temp_c'] - features['coolant_temp_mean_7d']\n","    )\n","\n","    features['brake_health_score'] = (\n","        features['brake_pad_mm'] - 10 * features['brake_wear_rate_30d']\n","    )\n","\n","    features['battery_health_indicator'] = (\n","        features['battery_voltage_v'] - 100 * features['battery_voltage_drop_rate_30d']\n","    )\n","\n","    features['temp_stability_score'] = 1 / (1 + features['engine_temp_std_7d'])\n","\n","    total_harsh = (\n","        features['total_harsh_braking_7d'] +\n","        features['total_harsh_acceleration_7d']\n","    )\n","    features['driving_aggression_score'] = total_harsh / 7\n","\n","    features['tire_balance_score'] = 1 / (1 + features['tire_pressure_std'])\n","\n","    return features\n","\n","\n","def get_feature_names() -> List[str]:\n","    return list(extract_features_from_vehicle.__annotations__.keys())\n","\n","\n","def features_to_vector(features: Dict) -> np.ndarray:\n","    names = [\n","        'engine_temp_c','oil_pressure_psi','brake_pad_mm','battery_voltage_v',\n","        'vibration_level','transmission_temp_c','tire_pressure_avg',\n","        'tire_pressure_std','tire_pressure_min','engine_temp_mean_7d',\n","        'engine_temp_std_7d','engine_temp_max_7d','engine_temp_min_7d',\n","        'oil_pressure_mean_7d','oil_pressure_std_7d','oil_pressure_min_7d',\n","        'battery_voltage_mean_7d','battery_voltage_min_7d',\n","        'coolant_temp_mean_7d','coolant_temp_max_7d',\n","        'vibration_mean_7d','vibration_max_7d',\n","        'transmission_temp_mean_7d','transmission_temp_max_7d',\n","        'brake_wear_rate_30d','oil_pressure_drop_rate_30d',\n","        'battery_voltage_drop_rate_30d','vibration_increase_rate_30d',\n","        'miles_since_last_service','avg_daily_miles_30d',\n","        'total_harsh_braking_7d','total_harsh_acceleration_7d',\n","        'total_cold_starts_7d','avg_fuel_consumption_7d',\n","        'avg_rpm_7d','max_rpm_7d','engine_stress_index',\n","        'cooling_efficiency','brake_health_score',\n","        'battery_health_indicator','temp_stability_score',\n","        'driving_aggression_score','tire_balance_score'\n","    ]\n","    return np.array([features[n] for n in names])\n","\n","\n","def prepare_training_data(telemetry_df: pd.DataFrame):\n","    X, y, vehicle_ids = [], [], []\n","\n","    for vid in telemetry_df['vehicle_id'].unique():\n","        vdf = telemetry_df[telemetry_df['vehicle_id'] == vid]\n","        try:\n","            feats = extract_features_from_vehicle(vdf)\n","            X.append(features_to_vector(feats))\n","            y.append(int(vdf.iloc[0]['is_failing']))\n","            vehicle_ids.append(vid)\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Skipping {vid}: {e}\")\n","\n","    return np.array(X), np.array(y), vehicle_ids\n","\n","\n","if __name__ == \"__main__\":\n","    print(\"üì• Loading training data from Google Drive...\")\n","    df = pd.read_csv(TRAIN_PATH)\n","    df['timestamp'] = pd.to_datetime(df['timestamp'])\n","\n","    X, y, vehicle_ids = prepare_training_data(df)\n","\n","    print(f\"‚úÖ Feature matrix shape: {X.shape}\")\n","    print(f\"‚úÖ Labels ‚Üí Failing: {y.sum()}, Normal: {(y==0).sum()}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ChyD9WgfO8q3","executionInfo":{"status":"ok","timestamp":1765772520897,"user_tz":-330,"elapsed":2202,"user":{"displayName":"Rudra Rajpurohit","userId":"01182349033982099683"}},"outputId":"f96f2b33-4ce4-4a3b-916a-9eb13142cd84"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["üì• Loading training data from Google Drive...\n","‚úÖ Feature matrix shape: (175, 43)\n","‚úÖ Labels ‚Üí Failing: 10, Normal: 165\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Train Isolation Forest model\n","\"\"\"\n","import numpy as np\n","import pandas as pd\n","from sklearn.ensemble import IsolationForest\n","from sklearn.metrics import (\n","    confusion_matrix,\n","    classification_report,\n","    roc_auc_score,\n","    precision_recall_curve,\n","    roc_curve\n",")\n","import joblib\n","import json\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","\n","\n","def train_isolation_forest(X_train, contamination=0.05):\n","    \"\"\"\n","    Train Isolation Forest model.\n","\n","    Args:\n","        X_train: Feature matrix\n","        contamination: Expected proportion of anomalies\n","\n","    Returns:\n","        Trained model\n","    \"\"\"\n","    print(f\"üèãÔ∏è  Training Isolation Forest...\")\n","    print(f\"   Samples: {X_train.shape[0]}\")\n","    print(f\"   Features: {X_train.shape[1]}\")\n","    print(f\"   Contamination: {contamination}\")\n","\n","    model = IsolationForest(\n","        n_estimators=100,\n","        max_samples=256,\n","        contamination=contamination,\n","        random_state=42,\n","        n_jobs=-1,\n","        verbose=1\n","    )\n","\n","    model.fit(X_train)\n","\n","    print(\"‚úÖ Training complete!\")\n","    return model\n","\n","\n","def transform_scores_to_risk(scores):\n","    \"\"\"\n","    Transform Isolation Forest anomaly scores to 0-1 risk scale.\n","\n","    Isolation Forest returns negative scores (more negative = more anomalous).\n","    We use sigmoid to convert to intuitive 0-1 risk scale.\n","    \"\"\"\n","    return 1 / (1 + np.exp(scores * 5))\n","\n","\n","def evaluate_model(model, X, y_true, dataset_name=\"Dataset\"):\n","    \"\"\"\n","    Comprehensive model evaluation.\n","\n","    Args:\n","        model: Trained Isolation Forest\n","        X: Features\n","        y_true: Ground truth labels (1=failing, 0=normal)\n","        dataset_name: Name for logging\n","\n","    Returns:\n","        Dictionary of metrics\n","    \"\"\"\n","    print(f\"\\nüìä Evaluating on {dataset_name}...\")\n","\n","    # Get predictions\n","    anomaly_scores = model.score_samples(X)\n","    risk_scores = transform_scores_to_risk(anomaly_scores)\n","    predictions = model.predict(X)  # -1 = anomaly, 1 = normal\n","\n","    # Convert to binary (1=anomaly, 0=normal)\n","    y_pred = (predictions == -1).astype(int)\n","\n","    # Confusion matrix\n","    cm = confusion_matrix(y_true, y_pred)\n","    tn, fp, fn, tp = cm.ravel()\n","\n","    # Metrics\n","    accuracy = (tp + tn) / (tp + tn + fp + fn)\n","    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n","    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n","    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","\n","    # ROC-AUC (using risk scores)\n","    roc_auc = roc_auc_score(y_true, risk_scores)\n","\n","    metrics = {\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1_score': f1,\n","        'roc_auc': roc_auc,\n","        'confusion_matrix': {\n","            'true_negatives': int(tn),\n","            'false_positives': int(fp),\n","            'false_negatives': int(fn),\n","            'true_positives': int(tp)\n","        },\n","        'n_samples': len(y_true),\n","        'n_failing': int(y_true.sum()),\n","        'n_detected': int(y_pred.sum())\n","    }\n","\n","    # Print report\n","    print(f\"\\n{'='*50}\")\n","    print(f\"  {dataset_name} Results\")\n","    print(f\"{'='*50}\")\n","    print(f\"  Samples: {metrics['n_samples']}\")\n","    print(f\"  Actual failing: {metrics['n_failing']}\")\n","    print(f\"  Detected as anomaly: {metrics['n_detected']}\")\n","    print(f\"\\n  Accuracy:  {accuracy:.3f}\")\n","    print(f\"  Precision: {precision:.3f}\")\n","    print(f\"  Recall:    {recall:.3f}\")\n","    print(f\"  F1 Score:  {f1:.3f}\")\n","    print(f\"  ROC-AUC:   {roc_auc:.3f}\")\n","    print(f\"\\n  Confusion Matrix:\")\n","    print(f\"    TN: {tn:3d}  |  FP: {fp:3d}\")\n","    print(f\"    FN: {fn:3d}  |  TP: {tp:3d}\")\n","    print(f\"{'='*50}\")\n","\n","    return metrics, risk_scores, y_pred\n","\n","\n","def plot_evaluation_results(y_true_train, risk_scores_train, y_pred_train,\n","                           y_true_val, risk_scores_val, y_pred_val):\n","    \"\"\"Create visualizations of model performance.\"\"\"\n","    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n","\n","    # 1. Risk Score Distribution (Train)\n","    axes[0, 0].hist(risk_scores_train[y_true_train == 0], bins=30, alpha=0.5, label='Normal', color='green')\n","    axes[0, 0].hist(risk_scores_train[y_true_train == 1], bins=30, alpha=0.5, label='Failing', color='red')\n","    axes[0, 0].set_xlabel('Risk Score')\n","    axes[0, 0].set_ylabel('Count')\n","    axes[0, 0].set_title('Train: Risk Score Distribution')\n","    axes[0, 0].legend()\n","    axes[0, 0].axvline(0.5, color='black', linestyle='--', label='Threshold')\n","\n","    # 2. Risk Score Distribution (Val)\n","    axes[0, 1].hist(risk_scores_val[y_true_val == 0], bins=30, alpha=0.5, label='Normal', color='green')\n","    axes[0, 1].hist(risk_scores_val[y_true_val == 1], bins=30, alpha=0.5, label='Failing', color='red')\n","    axes[0, 1].set_xlabel('Risk Score')\n","    axes[0, 1].set_ylabel('Count')\n","    axes[0, 1].set_title('Validation: Risk Score Distribution')\n","    axes[0, 1].legend()\n","    axes[0, 1].axvline(0.5, color='black', linestyle='--')\n","\n","    # 3. Confusion Matrix (Val)\n","    cm = confusion_matrix(y_true_val, y_pred_val)\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 2])\n","    axes[0, 2].set_xlabel('Predicted')\n","    axes[0, 2].set_ylabel('Actual')\n","    axes[0, 2].set_title('Validation: Confusion Matrix')\n","    axes[0, 2].set_xticklabels(['Normal', 'Failing'])\n","    axes[0, 2].set_yticklabels(['Normal', 'Failing'])\n","\n","    # 4. ROC Curve (Train)\n","    fpr_train, tpr_train, _ = roc_curve(y_true_train, risk_scores_train)\n","    roc_auc_train = roc_auc_score(y_true_train, risk_scores_train)\n","    axes[1, 0].plot(fpr_train, tpr_train, label=f'Train (AUC = {roc_auc_train:.3f})')\n","    axes[1, 0].plot([0, 1], [0, 1], 'k--', label='Random')\n","    axes[1, 0].set_xlabel('False Positive Rate')\n","    axes[1, 0].set_ylabel('True Positive Rate')\n","    axes[1, 0].set_title('Train: ROC Curve')\n","    axes[1, 0].legend()\n","    axes[1, 0].grid(True, alpha=0.3)\n","\n","    # 5. ROC Curve (Val)\n","    fpr_val, tpr_val, _ = roc_curve(y_true_val, risk_scores_val)\n","    roc_auc_val = roc_auc_score(y_true_val, risk_scores_val)\n","    axes[1, 1].plot(fpr_val, tpr_val, label=f'Val (AUC = {roc_auc_val:.3f})')\n","    axes[1, 1].plot([0, 1], [0, 1], 'k--', label='Random')\n","    axes[1, 1].set_xlabel('False Positive Rate')\n","    axes[1, 1].set_ylabel('True Positive Rate')\n","    axes[1, 1].set_title('Validation: ROC Curve')\n","    axes[1, 1].legend()\n","    axes[1, 1].grid(True, alpha=0.3)\n","\n","    # 6. Precision-Recall Curve (Val)\n","    precision, recall, _ = precision_recall_curve(y_true_val, risk_scores_val)\n","    axes[1, 2].plot(recall, precision)\n","    axes[1, 2].set_xlabel('Recall')\n","    axes[1, 2].set_ylabel('Precision')\n","    axes[1, 2].set_title('Validation: Precision-Recall Curve')\n","    axes[1, 2].grid(True, alpha=0.3)\n","\n","    plt.tight_layout()\n","    plt.savefig('logs/model_evaluation.png', dpi=150, bbox_inches='tight')\n","    print(\"‚úÖ Saved evaluation plot: logs/model_evaluation.png\")\n","    plt.close()\n","\n","\n","def save_model_and_metadata(model, metrics_train, metrics_val, X_train):\n","    \"\"\"Save model and metadata.\"\"\"\n","    # Save model\n","    model_path = 'models/isolation_forest_v1.pkl'\n","    joblib.dump(model, model_path)\n","    print(f\"‚úÖ Model saved: {model_path}\")\n","\n","    # Save metadata\n","    metadata = {\n","        'model_type': 'IsolationForest',\n","        'trained_at': datetime.now().isoformat(),\n","        'sklearn_version': joblib.__version__,\n","        'hyperparameters': {\n","            'n_estimators': model.n_estimators,\n","            'max_samples': model.max_samples,\n","            'contamination': model.contamination,\n","            'random_state': model.random_state\n","        },\n","        'training_data': {\n","            'n_samples': X_train.shape[0],\n","            'n_features': X_train.shape[1],\n","            'feature_names': get_feature_names()\n","        },\n","        'performance': {\n","            'train': metrics_train,\n","            'validation': metrics_val\n","        }\n","    }\n","\n","    metadata_path = 'models/model_metadata.json'\n","    with open(metadata_path, 'w') as f:\n","        json.dump(metadata, f, indent=2)\n","\n","    print(f\"‚úÖ Metadata saved: {metadata_path}\")\n","\n","\n","def main():\n","    \"\"\"Main training pipeline.\"\"\"\n","    print(\"=\" * 60)\n","    print(\"üöÄ ISOLATION FOREST TRAINING PIPELINE\")\n","    print(\"=\" * 60)\n","\n","    # Load data\n","    print(\"\\nüì• Loading datasets...\")\n","    train_df = pd.read_csv('/content/drive/MyDrive/iForestAutoAI/train_telemetry.csv')\n","    val_df = pd.read_csv('/content/drive/MyDrive/iForestAutoAI/val_telemetry.csv')\n","\n","    train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n","    val_df['timestamp'] = pd.to_datetime(val_df['timestamp'])\n","\n","    print(f\"‚úÖ Train: {len(train_df)} records, {train_df['vehicle_id'].nunique()} vehicles\")\n","    print(f\"‚úÖ Val:   {len(val_df)} records, {val_df['vehicle_id'].nunique()} vehicles\")\n","\n","    # Extract features\n","    print(\"\\nüîß Extracting features...\")\n","    X_train, y_train, train_vehicle_ids = prepare_training_data(train_df)\n","    X_val, y_val, val_vehicle_ids = prepare_training_data(val_df)\n","\n","    print(f\"‚úÖ Train features: {X_train.shape}\")\n","    print(f\"‚úÖ Val features:   {X_val.shape}\")\n","\n","    # Train model\n","    print(\"\\nüèãÔ∏è  Training model...\")\n","    model = train_isolation_forest(X_train, contamination=0.05)\n","\n","    # Evaluate\n","    print(\"\\nüìä Evaluating model...\")\n","    metrics_train, risk_scores_train, y_pred_train = evaluate_model(model, X_train, y_train, \"TRAIN\")\n","    metrics_val, risk_scores_val, y_pred_val = evaluate_model(model, X_val, y_val, \"VALIDATION\")\n","\n","    # Plot results\n","    print(\"\\nüìà Creating visualizations...\")\n","    plot_evaluation_results(\n","        y_train, risk_scores_train, y_pred_train,\n","        y_val, risk_scores_val, y_pred_val\n","    )\n","\n","    # Save model\n","    print(\"\\nüíæ Saving model...\")\n","    save_model_and_metadata(model, metrics_train, metrics_val, X_train)\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"üéâ TRAINING COMPLETE!\")\n","    print(\"=\" * 60)\n","    print(f\"\\nüìä Final Performance:\")\n","    print(f\"   Train F1:     {metrics_train['f1_score']:.3f}\")\n","    print(f\"   Val F1:       {metrics_val['f1_score']:.3f}\")\n","    print(f\"   Train ROC-AUC: {metrics_train['roc_auc']:.3f}\")\n","    print(f\"   Val ROC-AUC:   {metrics_val['roc_auc']:.3f}\")\n","\n","\n","if __name__ == \"__main__\":\n","    import os\n","    os.makedirs('logs', exist_ok=True)\n","    os.makedirs('models', exist_ok=True)\n","\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZGWJgPkGPmPV","executionInfo":{"status":"ok","timestamp":1765772881612,"user_tz":-330,"elapsed":5839,"user":{"displayName":"Rudra Rajpurohit","userId":"01182349033982099683"}},"outputId":"5804632e-4772-42f7-ab99-2e1e54c2725c"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","üöÄ ISOLATION FOREST TRAINING PIPELINE\n","============================================================\n","\n","üì• Loading datasets...\n","‚úÖ Train: 21000 records, 175 vehicles\n","‚úÖ Val:   4440 records, 37 vehicles\n","\n","üîß Extracting features...\n","‚úÖ Train features: (175, 43)\n","‚úÖ Val features:   (37, 43)\n","\n","üèãÔ∏è  Training model...\n","üèãÔ∏è  Training Isolation Forest...\n","   Samples: 175\n","   Features: 43\n","   Contamination: 0.05\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_iforest.py:336: UserWarning: max_samples (256) is greater than the total number of samples (175). max_samples will be set to n_samples for estimation.\n","  warn(\n","[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n","[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.4s finished\n","[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n","[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n","[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n","[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n","[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Training complete!\n","\n","üìä Evaluating model...\n","\n","üìä Evaluating on TRAIN...\n","\n","==================================================\n","  TRAIN Results\n","==================================================\n","  Samples: 175\n","  Actual failing: 10\n","  Detected as anomaly: 9\n","\n","  Accuracy:  0.994\n","  Precision: 1.000\n","  Recall:    0.900\n","  F1 Score:  0.947\n","  ROC-AUC:   1.000\n","\n","  Confusion Matrix:\n","    TN: 165  |  FP:   0\n","    FN:   1  |  TP:   9\n","==================================================\n","\n","üìä Evaluating on VALIDATION...\n","\n","==================================================\n","  VALIDATION Results\n","==================================================\n","  Samples: 37\n","  Actual failing: 1\n","  Detected as anomaly: 1\n","\n","  Accuracy:  1.000\n","  Precision: 1.000\n","  Recall:    1.000\n","  F1 Score:  1.000\n","  ROC-AUC:   1.000\n","\n","  Confusion Matrix:\n","    TN:  36  |  FP:   0\n","    FN:   0  |  TP:   1\n","==================================================\n","\n","üìà Creating visualizations...\n","‚úÖ Saved evaluation plot: logs/model_evaluation.png\n","\n","üíæ Saving model...\n","‚úÖ Model saved: models/isolation_forest_v1.pkl\n","‚úÖ Metadata saved: models/model_metadata.json\n","\n","============================================================\n","üéâ TRAINING COMPLETE!\n","============================================================\n","\n","üìä Final Performance:\n","   Train F1:     0.947\n","   Val F1:       1.000\n","   Train ROC-AUC: 1.000\n","   Val ROC-AUC:   1.000\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Test trained model on holdout test set\n","\"\"\"\n","import numpy as np\n","import pandas as pd\n","import joblib\n","import json\n","\n","\n","def analyze_failure_cases(model, X_test, y_test, vehicle_ids, test_df):\n","    \"\"\"Analyze false positives and false negatives.\"\"\"\n","\n","    anomaly_scores = model.score_samples(X_test)\n","    risk_scores = transform_scores_to_risk(anomaly_scores)\n","    predictions = (model.predict(X_test) == -1).astype(int)\n","\n","    # Find misclassifications\n","    false_positives = np.where((predictions == 1) & (y_test == 0))[0]\n","    false_negatives = np.where((predictions == 0) & (y_test == 1))[0]\n","\n","    print(f\"\\nüîç Analyzing Failure Cases...\")\n","    print(f\"   False Positives: {len(false_positives)}\")\n","    print(f\"   False Negatives: {len(false_negatives)}\")\n","\n","    # Show examples\n","    print(f\"\\n‚ùå False Positives (predicted failing, actually normal):\")\n","    for idx in false_positives[:3]:\n","        vehicle_id = vehicle_ids[idx]\n","        risk = risk_scores[idx]\n","        print(f\"   {vehicle_id}: risk={risk:.3f}\")\n","\n","    print(f\"\\n‚ùå False Negatives (predicted normal, actually failing):\")\n","    for idx in false_negatives[:3]:\n","        vehicle_id = vehicle_ids[idx]\n","        risk = risk_scores[idx]\n","        print(f\"   {vehicle_id}: risk={risk:.3f}\")\n","\n","\n","def test_on_individual_vehicles(model, test_df, n_examples=5):\n","    \"\"\"Test inference on individual vehicles (simulates production).\"\"\"\n","\n","    print(f\"\\nüß™ Testing Individual Vehicle Inference...\")\n","\n","    vehicles = test_df['vehicle_id'].unique()[:n_examples]\n","\n","    for vehicle_id in vehicles:\n","        vehicle_data = test_df[test_df['vehicle_id'] == vehicle_id]\n","        is_actually_failing = vehicle_data.iloc[0]['is_failing']\n","\n","        # Extract features\n","        features = extract_features_from_vehicle(vehicle_data)\n","        vector = features_to_vector(features).reshape(1, -1)\n","\n","        # Predict\n","        anomaly_score = model.score_samples(vector)[0]\n","        risk_score = transform_scores_to_risk(np.array([anomaly_score]))[0]\n","        prediction = \"FAILING\" if model.predict(vector)[0] == -1 else \"NORMAL\"\n","\n","        status = \"‚úÖ\" if (prediction == \"FAILING\" and is_actually_failing) or \\\n","                        (prediction == \"NORMAL\" and not is_actually_failing) else \"‚ùå\"\n","\n","        print(f\"\\n{status} {vehicle_id}:\")\n","        print(f\"   Predicted: {prediction} (risk={risk_score:.3f})\")\n","        print(f\"   Actual: {'FAILING' if is_actually_failing else 'NORMAL'}\")\n","        print(f\"   Top concerns:\")\n","\n","        # Show top anomalous features\n","        concerns = []\n","        if features['brake_pad_mm'] < 4.0:\n","            concerns.append(f\"Low brake pads ({features['brake_pad_mm']:.1f}mm)\")\n","        if features['engine_temp_max_7d'] > 100:\n","            concerns.append(f\"High engine temp ({features['engine_temp_max_7d']:.1f}¬∞C)\")\n","        if features['battery_voltage_v'] < 12.2:\n","            concerns.append(f\"Low battery ({features['battery_voltage_v']:.2f}V)\")\n","        if features['oil_pressure_min_7d'] < 30:\n","            concerns.append(f\"Low oil pressure ({features['oil_pressure_min_7d']:.1f} psi)\")\n","\n","        for concern in concerns[:3]:\n","            print(f\"     - {concern}\")\n","\n","\n","def main():\n","    \"\"\"Test pipeline.\"\"\"\n","    print(\"=\" * 60)\n","    print(\"üß™ MODEL TESTING ON HOLDOUT SET\")\n","    print(\"=\" * 60)\n","\n","    # Load model\n","    print(\"\\nüì• Loading trained model...\")\n","    model = joblib.load('/content/models/isolation_forest_v1.pkl')\n","\n","    with open('models/model_metadata.json') as f:\n","        metadata = json.load(f)\n","\n","    print(f\"‚úÖ Loaded model trained on {metadata['trained_at']}\")\n","    print(f\"   Train F1: {metadata['performance']['train']['f1_score']:.3f}\")\n","    print(f\"   Val F1:   {metadata['performance']['validation']['f1_score']:.3f}\")\n","\n","    # Load test data\n","    print(\"\\nüì• Loading test data...\")\n","    test_df = pd.read_csv('/content/drive/MyDrive/iForestAutoAI/test_telemetry.csv')\n","    test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n","    print(f\"‚úÖ Test: {len(test_df)} records, {test_df['vehicle_id'].nunique()} vehicles\")\n","\n","    # Extract features\n","    print(\"\\nüîß Extracting features...\")\n","    X_test, y_test, test_vehicle_ids = prepare_training_data(test_df)\n","    print(f\"‚úÖ Test features: {X_test.shape}\")\n","\n","    # Evaluate\n","    print(\"\\nüìä Evaluating on test set...\")\n","    metrics_test, risk_scores, y_pred = evaluate_model(model, X_test, y_test, \"TEST\")\n","\n","    # Analyze failures\n","    analyze_failure_cases(model, X_test, y_test, test_vehicle_ids, test_df)\n","\n","    # Test individual vehicles\n","    test_on_individual_vehicles(model, test_df, n_examples=5)\n","\n","    # Save test results\n","    test_results = {\n","        'tested_at': datetime.now().isoformat(),\n","        'test_set_size': len(X_test),\n","        'metrics': metrics_test\n","    }\n","\n","    with open('logs/test_results.json', 'w') as f:\n","        json.dump(test_results, f, indent=2)\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"üéâ TESTING COMPLETE!\")\n","    print(\"=\" * 60)\n","    print(f\"\\nüìä Test Performance:\")\n","    print(f\"   Accuracy:  {metrics_test['accuracy']:.3f}\")\n","    print(f\"   Precision: {metrics_test['precision']:.3f}\")\n","    print(f\"   Recall:    {metrics_test['recall']:.3f}\")\n","    print(f\"   F1 Score:  {metrics_test['f1_score']:.3f}\")\n","    print(f\"   ROC-AUC:   {metrics_test['roc_auc']:.3f}\")\n","\n","\n","if __name__ == \"__main__\":\n","    from datetime import datetime\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JSDQhqzRQR29","executionInfo":{"status":"ok","timestamp":1765772985149,"user_tz":-330,"elapsed":623,"user":{"displayName":"Rudra Rajpurohit","userId":"01182349033982099683"}},"outputId":"90822c33-f96a-4058-ebad-de3caa6b6616"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","üß™ MODEL TESTING ON HOLDOUT SET\n","============================================================\n","\n","üì• Loading trained model...\n","‚úÖ Loaded model trained on 2025-12-15T04:28:01.452342\n","   Train F1: 0.947\n","   Val F1:   1.000\n","\n","üì• Loading test data...\n","‚úÖ Test: 4560 records, 38 vehicles\n","\n","üîß Extracting features...\n","‚úÖ Test features: (38, 43)\n","\n","üìä Evaluating on test set...\n","\n","üìä Evaluating on TEST...\n","\n","==================================================\n","  TEST Results\n","==================================================\n","  Samples: 38\n","  Actual failing: 1\n","  Detected as anomaly: 1\n","\n","  Accuracy:  1.000\n","  Precision: 1.000\n","  Recall:    1.000\n","  F1 Score:  1.000\n","  ROC-AUC:   1.000\n","\n","  Confusion Matrix:\n","    TN:  37  |  FP:   0\n","    FN:   0  |  TP:   1\n","==================================================\n","\n","üîç Analyzing Failure Cases...\n","   False Positives: 0\n","   False Negatives: 0\n","\n","‚ùå False Positives (predicted failing, actually normal):\n","\n","‚ùå False Negatives (predicted normal, actually failing):\n","\n","üß™ Testing Individual Vehicle Inference...\n","\n","‚úÖ VEH_0001:\n","   Predicted: NORMAL (risk=0.901)\n","   Actual: NORMAL\n","   Top concerns:\n","\n","‚úÖ VEH_0007:\n","   Predicted: FAILING (risk=0.964)\n","   Actual: FAILING\n","   Top concerns:\n","     - High engine temp (120.0¬∞C)\n","     - Low battery (11.89V)\n","     - Low oil pressure (17.6 psi)\n"]},{"output_type":"stream","name":"stderr","text":["[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n","[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n","[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n","[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n","[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n","[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n","[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n","[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n","[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n"]},{"output_type":"stream","name":"stdout","text":["\n","‚úÖ VEH_0030:\n","   Predicted: NORMAL (risk=0.886)\n","   Actual: NORMAL\n","   Top concerns:\n","     - High engine temp (103.4¬∞C)\n","\n","‚úÖ VEH_0037:\n","   Predicted: NORMAL (risk=0.926)\n","   Actual: NORMAL\n","   Top concerns:\n","     - High engine temp (102.5¬∞C)\n","\n","‚úÖ VEH_0038:\n","   Predicted: NORMAL (risk=0.908)\n","   Actual: NORMAL\n","   Top concerns:\n","     - High engine temp (101.0¬∞C)\n","\n","============================================================\n","üéâ TESTING COMPLETE!\n","============================================================\n","\n","üìä Test Performance:\n","   Accuracy:  1.000\n","   Precision: 1.000\n","   Recall:    1.000\n","   F1 Score:  1.000\n","   ROC-AUC:   1.000\n"]},{"output_type":"stream","name":"stderr","text":["[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n","[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n","[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n","[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n","[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n","[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n","[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"]}]},{"cell_type":"code","source":["import shutil\n","import os\n","\n","os.makedirs(\"/content/drive/MyDrive/iForest/logs\", exist_ok=True)\n","os.makedirs(\"/content/drive/MyDrive/iForest/models\", exist_ok=True)\n","\n","shutil.move(\"/content/logs/model_evaluation.png\",\n","            \"/content/drive/MyDrive/iForest/logs/model_evaluation.png\")\n","\n","shutil.move(\"/content/models/isolation_forest_v1.pkl\",\n","            \"/content/drive/MyDrive/iForest/models/isolation_forest_v1.pkl\")\n","\n","shutil.move(\"/content/models/model_metadata.json\",\n","            \"/content/drive/MyDrive/iForest/models/model_metadata.json\")\n","\n","print(\"‚úÖ Files moved to Google Drive\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2nN_MddtQSR6","executionInfo":{"status":"ok","timestamp":1765773234825,"user_tz":-330,"elapsed":25,"user":{"displayName":"Rudra Rajpurohit","userId":"01182349033982099683"}},"outputId":"5b083cc9-ea5e-4d0c-d4d9-724e75c174d8"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Files moved to Google Drive\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_e0RgI19SL6b"},"execution_count":null,"outputs":[]}]}